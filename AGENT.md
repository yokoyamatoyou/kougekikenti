### **プロジェクト名：オールインワン誹謗中傷証拠収集ツール (v2.0)**

### **1\. プロジェクト概要**

#### **1.1. 現状 (Current State)**

攻撃性判定.pyは、既存のExcelファイルを読み込み、OpenAIのAPIを利用してテキストの攻撃性を詳細に分析・スコアリングする、非常に高機能なGUIデスクトップアプリケーションである。

#### **1.2. 開発目標 (Target State)**

このアプリケーションを、\*\*データ収集から分析までを一気通貫で行える「オールインワン・ツール」\*\*に進化させる。ユーザーがX(旧Twitter)のユーザーIDを入力するだけで、投稿の収集、AIによる分析、そして最終的な証拠目録（Excelファイル）の出力までを、単一のインターフェースで完結させることを目指す。

### **2\. 提案するディレクトリ構造**

保守性と拡張性を高めるため、以下のディレクトリ構造を採用する。

aggression\_analyzer/  
├── main.py                 \# アプリケーションのメインエントリーポイント  
├── requirements.txt        \# 依存ライブラリリスト  
├── .env.example            \# 環境変数設定のテンプレート  
├── .gitignore              \# Git管理対象外ファイル  
├── README.md               \# プロジェクトの説明書  
│  
├── config/  
│   └── settings.py         \# 設定ファイル（モデル名、プロンプト、スコア重み付け等）  
│  
├── gui/  
│   ├── \_\_init\_\_.py  
│   └── app.py              \# GUIアプリケーション本体のクラス  
│  
├── modules/  
│   ├── \_\_init\_\_.py  
│   ├── scraper.py          \# X(旧Twitter) スクレイピングモジュール  
│   └── analyzer.py         \# OpenAI 分析モジュール  
│  
└── output/                 \# 分析結果のExcelファイルを保存するデフォルトディレクトリ  
    └── .gitkeep

### **3\. AIエージェント向け 高詳細開発指示**

このプロジェクトは、以下のフェーズに分けて開発を進めること。各タスク完了後、指定された「検証ステップ」を実行し、進捗ログを更新すること。

### **【フェーズ0】 プロジェクトのセットアップと高度なリファクタリング**

**目的:** 既存コードを専門的な構造に整理し、堅牢性と保守性を向上させる。

* **タスク 0.1: ディレクトリとファイルの作成**  
  * 上記のディレクトリ構造案に基づき、必要なディレクトリと空のPythonファイル（\_\_init\_\_.pyを含む）を作成する。  
* **タスク 0.2: 依存関係と環境変数の設定**  
  * requirements.txt を作成し、内容を記述する: openai, pandas, customtkinter, openpyxl, ntscraper, python-dotenv。
  * .env.example を作成し、OPENAI\_API\_KEY="YOUR\_API\_KEY\_HERE" と記述する。  
  * main.py の先頭で dotenv を使って環境変数を読み込むようにする。  
* **タスク 0.3: 設定ファイルの作成 (config/settings.py)**  
  * 分析に使用する定数を一元管理する。  
    \# config/settings.py  
    \# OpenAI Models  
    MODERATION\_MODEL \= "text-moderation-latest"  
    AGGRESSION\_ANALYSIS\_MODEL \= "gpt-4o-mini" \# gpt-4.1-mini-2025-04-14 はAPIに存在しないため、gpt-4o-miniを使用

    \# Analysis Parameters  
    DEFAULT\_TEMPERATURE \= 0.5  
    DEFAULT\_TOP\_P \= 1.0

    \# Aggression Score Weights  
    \# (ここに総合攻撃性スコアの重み付けを定義)  
    WEIGHTS \= {  
        "hate\_score": 0.5,  
        \# ...  
    }

* **タスク 0.4: ロジックの分離とクラス化**  
  * 攻撃性判定.py のロジックを、新しいディレクトリ構造に合わせて以下の通りに分離・移動する。  
    * **gui/app.py**: customtkinter関連のUIコード全てを ModerationApp クラスとしてここに移動。  
    * **modules/analyzer.py**: moderate\_text, get\_aggressiveness\_score, total\_aggression計算ロジックを Analyzer クラスのメソッドとしてここに移動。  
    * **main.py**: アプリケーションを起動する数行のコードのみを記述。  
      \# main.py  
      from gui.app import ModerationApp  
      from dotenv import load\_dotenv

      if \_\_name\_\_ \== "\_\_main\_\_":  
          load\_dotenv()  
          app \= ModerationApp()  
          app.mainloop()

* **タスク 0.5: 【重要】攻撃性スコア判定プロンプトの高度化**  
  * modules/analyzer.py の get\_aggressiveness\_score 関数を修正する。  
  * **目的:** AIの応答を安定させ、プログラムでの処理を容易にするため、**JSON形式での出力を強制する**。  
  * **修正後のプロンプト案:**  
    \# config/settings.py にプロンプトを定義  
    AGGRESSION\_PROMPT\_TEMPLATE \= """  
    あなたはソーシャルメディアの投稿を分析し、その攻撃性を評価する専門家です。  
    以下の基準に従って、与えられた文章の攻撃性スコアを決定し、その理由を説明してください。

    \# 評価基準:  
    0: 攻撃性なし。  
    1-3: 軽度の攻撃性（例: 皮肉、無神経な表現）。  
    4-6: 中程度の攻撃性（例: 明確な批判、侮辱）。  
    7-9: 高度の攻撃性（例: 強い敵意、脅迫）。  
    10: 極度の攻撃性（例: 暴力の助長、深刻な人権侵害）。

    \# 分析対象の文章:  
    \---  
    {text}  
    \---

    \# 出力形式:  
    必ず以下のJSON形式で回答してください。  
    {{  
      "score": \[0-10の整数\],  
      "reason": "なぜそのスコアを付けたのかを40文字程度で具体的に説明"  
    }}  
    """

  * **API呼び出しの修正:**  
    * client.chat.completions.create の response\_format パラメータに {"type": "json\_object"} を指定し、JSONモードを有効にする。  
    * 返ってきたJSON文字列を json.loads() でパースしてスコアと理由を抽出する。これにより、不安定な文字列分割が不要になる。  
* **【検証ステップ 0】**  
  * pip install \-r requirements.txt を実行。  
  * main.py を実行し、既存のExcelファイルを読み込んで分析する機能が、リファクタリング前と全く同じように動作することを確認する。

### **【フェーズ1】 データ収集機能の堅牢な実装**

**目的:** X(旧Twitter)から投稿データをスクレイピングする機能を、エラー処理を含めて堅牢に実装する。

* **タスク 1.1: スクレイパーモジュールの作成 (modules/scraper.py)**  
  * Scraper クラスを定義する。  
  * scrape\_user\_posts(self, username: str, limit: int) メソッドを実装する。  
  * ntscraper を使用し、投稿の日時・URL・本文を収集し、pandasのDataFrameとして返す。
  * **エラーハンドリング:** try-exceptブロックを用いて、ユーザーが存在しない場合や投稿が取得できない場合に、空のDataFrameを返すか、カスタム例外を送出するように実装する。  
* **【検証ステップ 1】**  
  * scraper.py を直接実行した際にテスト用のユーザーIDで動作確認ができるよう、if \_\_name\_\_ \== "\_\_main\_\_": ブロックにテストコードを記述する。

### **【フェーズ2】 UI/UXの刷新と非同期処理の導入**

**目的:** 新機能と既存機能を統合し、UIの応答性を維持するために非同期処理を導入する。

* **タスク 2.1: GUIのレイアウト変更 (gui/app.py)**  
  * 既存の「ファイルを選択」ボタンを削除し、「XユーザーID」と「取得件数」の入力欄を設置する。  
* **タスク 2.2: 【重要】非同期処理の実装**  
  * **目的:** データ収集やAI分析のような時間のかかる処理中にGUIが固まるのを防ぐ。  
  * **実装:**  
    1. threading ライブラリをインポートする。  
    2. 「収集＆分析開始」ボタンがクリックされた際に、実際の処理（収集と分析）を行う新しいメソッド \_run\_analysis\_thread を定義する。  
    3. このメソッドを threading.Thread のターゲットとして指定し、バックグラウンドで実行する。  
    4. 処理中はメインボタンを無効化 (state="disabled") し、プログレスバーを動かす。  
    5. 処理完了後、バックグラウンドスレッドからGUIの状態（ボタンの有効化、完了メッセージ表示）を更新する。self.after() を使ってスレッドセーフなUI更新を行うこと。  
* **タスク 2.3: メインロジックの統合 (gui/app.py)**  
  * 「収集＆分析開始」ボタンのロジックを、ScraperクラスとAnalyzerクラスを順に呼び出すように修正する。  
  * プログレスバーとステータスラベルが、収集と分析の各ステップの進捗をリアルタイムに表示するように更新する。  
* **【検証ステップ 2】**  
  * アプリケーションを起動し、テスト用ユーザーIDで「収集＆分析開始」ボタンをクリックする。  
  * 処理中にGUIがフリーズせず、プログレスバーとステータス表示が正常に更新されることを確認する。  
  * 最終的に分析結果のExcelが output/ ディレクトリに保存されることを確認する。

### **【フェーズ3】 最終化とドキュメント整備**

**目的:** ツールの完成度を高め、高品質なドキュメントを作成する。

* **タスク 3.1: コードのクリーンアップと型ヒントの追加**  
  * プロジェクト全体のコードを見直し、Pythonの型ヒント (str, int, pd.DataFrameなど) を全ての関数・メソッドに追加する。これにより、コードの可読性と信頼性が向上する。  
* **タスク 3.2: README.md の作成**  
  * プロジェクトの目的、新しいディレクトリ構造、セットアップ方法（APIキー設定含む）、使い方を詳細に記述する。  
  * 各機能のスクリーンショット画像を含めると、より分かりやすくなる。  
* **【検証ステップ 3】**  
  * README.md の指示のみを頼りに、全く新しい環境でプロジェクトのセットアップと実行が問題なく行えることを確認する。

### **AIエージェント進捗ログ (AI Agent Progress Log)**

*AIエージェントは、各タスク完了後にこのリストを更新すること。*
* [x] **Phase 0**
  * [x] Task 0.1: ディレクトリとファイルの作成
  * [x] Task 0.2: 依存関係と環境変数の設定
  * [x] Task 0.3: 設定ファイルの作成
  * [x] Task 0.4: ロジックの分離とクラス化
* [x] Task 0.5: 攻撃性スコア判定プロンプトの高度化
* [x] Verification 0
* [x] **Phase 1**
* [x] Task 1.1: スクレイパーモジュールの作成
* [x] Verification 1
* [x] **Phase 2**
* [x] Task 2.1: GUIのレイアウト変更
* [x] Task 2.2: 非同期処理の実装
* [x] Task 2.3: メインロジックの統合
* [x] Verification 2
* [x] **Phase 3**
  * [x] Task 3.1: コードのクリーンアップと型ヒントの追加
  * [x] Task 3.2: README.md の作成
  * [x] Verification 3
* [x] 2025-07-11 現行の実装がAGENT.mdと一致することを確認

### **補足: PR作成エラーへの対処**

稀にCodexが"Failed to create PR"と出力し、プルリクエストの作成に失敗する場合がある。
多くは `__pycache__` や `.pytest_cache` などのキャッシュファイルがGit管理下に入って
しまうことが原因である。次の手順で対処すること。

1. `git status --short` で不要なファイルが無いか確認する。
2. `.gitignore` に `__pycache__/` と `.pytest_cache/` を含め、既に存在するキャッシュ
   ディレクトリは `git clean -fd` で削除する。
3. 変更をコミットし直してからPRを作成する。
